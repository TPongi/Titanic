{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNVBmiaw/FcgmA3USZtWEer"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"qQU3ManTX9_4","executionInfo":{"status":"ok","timestamp":1674204589387,"user_tz":-60,"elapsed":99467,"user":{"displayName":"tim pongratz","userId":"18291608650973378601"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"843257fd-b11f-49f7-f9de-0cad805d95ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Kaggle/Old_Titanic/Titanic\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Kaggle/Old_Titanic/Titanic"]},{"cell_type":"code","source":["df = pd.read_csv('train.csv')\n","dftest = df[['Fare','SibSp']]\n","testarr = dftest.to_numpy()\n","X=testarr[0:5,:].T\n","Y = df['Survived'].to_numpy()\n","Y = Y[0:5]\n","X.shape\n","X"],"metadata":{"id":"W4V4FSZTYCYa","executionInfo":{"status":"ok","timestamp":1674207080673,"user_tz":-60,"elapsed":231,"user":{"displayName":"tim pongratz","userId":"18291608650973378601"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"13d889d7-246f-4f7a-dc58-8329b27cf175"},"execution_count":79,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 7.25  , 71.2833,  7.925 , 53.1   ,  8.05  ],\n","       [ 1.    ,  1.    ,  0.    ,  1.    ,  0.    ]])"]},"metadata":{},"execution_count":79}]},{"cell_type":"markdown","source":["Start with logistic regression and gradient descent based on the first section of the deep learning and \n","\n","\n"],"metadata":{"id":"iHj_VpWuYkUb"}},{"cell_type":"code","source":["class log_reg:\n","    def __init__(self):\n","        self.W=[] # Weights\n","        self.b=[] # Biases\n","        self.z=np.array([0, 2]) # Linear term)\n","        self.dz=0 # Derivative of loss w.r.t. Z\n","        self.dW=0 \n","        self.db =0 \n","        self.loss=0\n","        self.A =0\n","        self.Y_predict =0\n","        self.score=0\n","\n","    def initialize_parm(self,X):\n","\n","        self.W = np.random.rand(X.shape[0],1) # Number of features\n","        \n","        self.b = 0\n","    \n","    def sigmoid(self):\n","        A=1 / (1 + np.exp(-self.z))\n","        return A\n","\n","    def forward_prop(self,X,Y):\n","        m = X.shape[1]\n","        self.z = np.dot(self.W.T,X) + self.b # Vectorized implementation\n","\n","        self.A = self.sigmoid()\n","        \n","        self.loss = -(1/m)*np.sum(Y*np.log(self.A)+(1-Y)*np.log(1-self.A)) # Cost function for logistic regressopn\n","    \n","    def backward_prop(self,X,Y):\n","        m = X.shape[1]\n","        dz = self.A - Y # Derivative of cost function w.r.t. dz \n","        #print(dz)\n","        '''\n","        dA = -Y/A + (1-y)/(1-a) applying chain rule leads to dZ = A-Y\n","        '''\n","        self.dW = (1/m)*np.dot(X,(self.A-Y).T)\n","\n","        self.db = (1/m)*np.sum(self.A-Y)\n","\n","\n","    def update_parm(self,learning_rate):\n","        self.W = self.W - learning_rate * self.dW\n","        self.b = self.b - learning_rate * self.db\n","\n","    def train(self,X,Y,num_it=10000,learning_rate=0.01):\n","        # X should be in shape (n_x,m)\n","\n","        self.initialize_parm(X)\n","        for ii in range(num_it):\n","            self.forward_prop(X,Y)\n","            #print('Loss after ' +str(ii) + 'th iteration: ' + str(self.loss))\n","            self.backward_prop(X,Y)\n","            self.update_parm(learning_rate)\n","            \n","\n","    def predict(self,X):\n","        self.z = np.dot(self.W.T,X) + self.b # Vectorized implementation\n","        A_temp = self.sigmoid()\n","        self.Y_predict = np.where(A_temp <= 0.5,0,1)\n","        return self.Y_predict\n","  "],"metadata":{"id":"w0aqM0cuYhmx","executionInfo":{"status":"ok","timestamp":1674210996074,"user_tz":-60,"elapsed":305,"user":{"displayName":"tim pongratz","userId":"18291608650973378601"}}},"execution_count":178,"outputs":[]},{"cell_type":"code","source":["logreg_model = log_reg()\n","logreg_model.train(X,Y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dfGDi8LSeF30","executionInfo":{"status":"ok","timestamp":1674206956054,"user_tz":-60,"elapsed":4,"user":{"displayName":"tim pongratz","userId":"18291608650973378601"}},"outputId":"47ad85ae-44f6-4763-b9cd-f5bc67b5803c"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.90539949 0.99999994 0.85570952 0.99999648 0.85914174]]\n","Loss after 0th iteration: -0.8947842919947999\n","[[0.61106868 0.27605499 0.45630183 0.36307529 0.45561618]]\n","Loss after 1th iteration: -0.9274708502408283\n","[[0.99999547 1.         0.99999714 1.         0.99999766]]\n","Loss after 2th iteration: nan\n","[[0.99995749 1.         0.99996766 1.         0.99997253]]\n","Loss after 3th iteration: nan\n","[[0.99960144 1.         0.99963462 1.         0.99967749]]\n","Loss after 4th iteration: nan\n","[[0.99627879 1.         0.99589117 1.         0.99623074]]\n","Loss after 5th iteration: nan\n","[[0.96657926 1.         0.95604737 1.         0.95804394]]\n","Loss after 6th iteration: -1.322931735198666\n","[[0.77926165 0.99956502 0.69035766 0.99726839 0.69305451]]\n","Loss after 7th iteration: -0.6131156425835647\n","[[0.49975651 0.01061609 0.36329014 0.03742464 0.36124544]]\n","Loss after 8th iteration: -1.996851852682362\n","[[0.99999997 1.         0.99999999 1.         0.99999999]]\n","Loss after 9th iteration: nan\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-74-02d81b94aa05>:27: RuntimeWarning: divide by zero encountered in log\n","  self.loss = (1/m)*np.sum(Y*np.log(self.A)+(1-Y)*np.log(1-self.A)) # Cost function for logistic regressopn\n","<ipython-input-74-02d81b94aa05>:27: RuntimeWarning: invalid value encountered in multiply\n","  self.loss = (1/m)*np.sum(Y*np.log(self.A)+(1-Y)*np.log(1-self.A)) # Cost function for logistic regressopn\n"]}]},{"cell_type":"code","source":["logreg_model.predict(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MJbiOJ2bfIky","executionInfo":{"status":"ok","timestamp":1674206985841,"user_tz":-60,"elapsed":240,"user":{"displayName":"tim pongratz","userId":"18291608650973378601"}},"outputId":"087f6b1d-ae00-46e1-cb34-171060bb445c"},"execution_count":78,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1, 1, 1, 1, 1]])"]},"metadata":{},"execution_count":78}]},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","clf = LogisticRegression().fit(X.T, Y)\n","clf.predict(X.T)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DIbNkMBSfKPJ","executionInfo":{"status":"ok","timestamp":1674207676720,"user_tz":-60,"elapsed":230,"user":{"displayName":"tim pongratz","userId":"18291608650973378601"}},"outputId":"57ca1097-68ff-4547-8adf-cea986b14b7f"},"execution_count":94,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 0, 1, 0])"]},"metadata":{},"execution_count":94}]},{"cell_type":"code","source":["from sklearn.datasets import load_breast_cancer\n","Xt, yt = load_breast_cancer(return_X_y=True)\n","clf = LogisticRegression().fit(Xt, yt)\n","clf.predict(Xt)\n","clf.score(Xt, yt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WtF2_i4jYCvR","executionInfo":{"status":"ok","timestamp":1674210859906,"user_tz":-60,"elapsed":371,"user":{"displayName":"tim pongratz","userId":"18291608650973378601"}},"outputId":"306ed65b-a97e-4231-c9e7-30287c4f15e7"},"execution_count":168,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"execute_result","data":{"text/plain":["0.9490333919156415"]},"metadata":{},"execution_count":168}]},{"cell_type":"code","source":["yT =yt.T.reshape(1,yt.shape[0])\n","Xt.T.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f3rpwO_QgIw5","executionInfo":{"status":"ok","timestamp":1674210943642,"user_tz":-60,"elapsed":5,"user":{"displayName":"tim pongratz","userId":"18291608650973378601"}},"outputId":"465d2208-c93c-474c-efa5-d50dca8c2036"},"execution_count":170,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(30, 569)"]},"metadata":{},"execution_count":170}]},{"cell_type":"code","source":["iris_model = log_reg()\n","iris_model.train(Xt.T,yT)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k6JLkaKbhiG4","executionInfo":{"status":"ok","timestamp":1674211005116,"user_tz":-60,"elapsed":1884,"user":{"displayName":"tim pongratz","userId":"18291608650973378601"}},"outputId":"6cece3a7-9881-4415-e905-510580ad7ea2"},"execution_count":180,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-178-82c641617481>:29: RuntimeWarning: divide by zero encountered in log\n","  self.loss = -(1/m)*np.sum(Y*np.log(self.A)+(1-Y)*np.log(1-self.A)) # Cost function for logistic regressopn\n","<ipython-input-178-82c641617481>:29: RuntimeWarning: invalid value encountered in multiply\n","  self.loss = -(1/m)*np.sum(Y*np.log(self.A)+(1-Y)*np.log(1-self.A)) # Cost function for logistic regressopn\n","<ipython-input-178-82c641617481>:20: RuntimeWarning: overflow encountered in exp\n","  A=1 / (1 + np.exp(-self.z))\n"]}]},{"cell_type":"code","source":["trainpredict=iris_model.predict(Xt.T)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c3uafzrohnBB","executionInfo":{"status":"ok","timestamp":1674211006665,"user_tz":-60,"elapsed":664,"user":{"displayName":"tim pongratz","userId":"18291608650973378601"}},"outputId":"42f824bf-38c8-416c-9ed2-bce73a516eb2"},"execution_count":181,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-178-82c641617481>:20: RuntimeWarning: overflow encountered in exp\n","  A=1 / (1 + np.exp(-self.z))\n"]}]},{"cell_type":"code","source":["100 - np.mean(np.abs(trainpredict- yT)) * 100"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wBBDw-acp1rv","executionInfo":{"status":"ok","timestamp":1674211007944,"user_tz":-60,"elapsed":228,"user":{"displayName":"tim pongratz","userId":"18291608650973378601"}},"outputId":"3cb978d5-caf7-4c1f-f944-366af21fe93a"},"execution_count":182,"outputs":[{"output_type":"execute_result","data":{"text/plain":["89.63093145869948"]},"metadata":{},"execution_count":182}]},{"cell_type":"code","source":[],"metadata":{"id":"ahDCuuxJp26T"},"execution_count":null,"outputs":[]}]}